{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOvxs3lGrN14gI6I/SQ6fU8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KKAARRIIMM15/NLP/blob/main/Retrieval-Augmented%20Generation/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtHP4UR_t8xE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install wikipedia sentence-transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-1.3B\", device_map=\"auto\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSfp2SXYt_SH",
        "outputId": "fe86af05-e980-4a9d-99ab-f93b6ddb97d8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#topic = \"Artificial intelligence\"\n",
        "#topic = \"United States\"\n",
        "topic = \"Deep Learning\"\n",
        "wiki_content = wikipedia.page(topic).content\n",
        "\n",
        "def chunk_text(text, chunk_size=200, overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = \" \".join(words[i:i+chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "def retrieve_top_k(query, k=6):\n",
        "    query_embedding = model.encode([query])\n",
        "    D, I = index.search(query_embedding, k)\n",
        "    return [chunks[i] for i in I[0]]\n",
        "\n",
        "def rag_query(query, pipe):\n",
        "    top_chunks = retrieve_top_k(query)\n",
        "    #print(\"chuuuuuung\")\n",
        "    #print(top_chunks)\n",
        "    context = \"\\n\\n\".join(top_chunks)\n",
        "    prompt = f\"\"\"Use the context below to answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    response = pipe(prompt, max_new_tokens=300, do_sample=True, temperature=0.7)\n",
        "    return response[0]['generated_text']\n",
        "\n",
        "chunks = chunk_text(wiki_content)\n",
        "#print(len(chunks[1].split(\" \") ))\n",
        "print(\"\\n\")\n",
        "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = model.encode(chunks, convert_to_numpy=True)\n",
        "\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n",
        "\n",
        "#query = \"What are the main goals of Artificial Intelligence?\"\n",
        "query = \"Where are the types of Deep Learning?\"\n",
        "#query = \"What is the GDP of USA?\"\n",
        "print(\"\\n\")\n",
        "answer = rag_query(query, pipe)\n",
        "print(\"\\n\" + answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWT4RlHUt_Uh",
        "outputId": "d89d9c4d-b93c-4922-e487-f8c5c207771e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Use the context below to answer the question.\n",
            "\n",
            "Context:\n",
            "Deep learning is a subset of machine learning that focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be either supervised, semi-supervised or unsupervised. Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance. Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose. == Overview == Most modern\n",
            "\n",
            "Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose. == Overview == Most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines. Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features\n",
            "\n",
            "and output layers. There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions. These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm. For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name \"deep\" networks. DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network. For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks. Deep architectures include many variants of a few basic approaches. Each\n",
            "\n",
            "composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network. For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks. Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets. DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights. That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data. Recurrent neural networks, in which data can flow in any direction, are used for applications such as language modeling. Long short-term memory is particularly effective for this use.\n",
            "\n",
            "ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3. The success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs. In 2014, the state of the art was training “very deep neural network” with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem. In 2015, two techniques were developed to train very deep networks: the Highway Network was published in May 2015, and the residual neural network (ResNet) in Dec 2015. ResNet behaves like an open-gated Highway Net. Around the same time, deep learning started impacting the field of art. Early examples included Google DeepDream (2015), and neural style transfer (2015), both of which were based on pretrained image classification neural networks, such as VGG-19. Generative adversarial network (GAN) by (Ian Goodfellow et al., 2014) (based on Jürgen Schmidhuber's principle of artificial curiosity) became state of the art in generative modeling during 2014-2018 period. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based\n",
            "\n",
            "are able to extract better features than shallow models and hence, extra layers help in learning the features effectively. Deep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance. Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data is more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks. The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons. Although the history of its appearance is apparently more complicated. == Interpretations == Deep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference. The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent\n",
            "\n",
            "Question:\n",
            "Where are the types of Deep Learning?\n",
            "\n",
            "Answer:\n",
            "The types of Deep Learning are:\n",
            "\n",
            "Fully connected networks are a sub-type of deep neural networks where\n",
            "\n",
            "the networks are composed of a small number of neurons,\n",
            "\n",
            "the connections between neurons are in the form of weighted adjacency matrices, and\n",
            "\n",
            "the network is trained to minimize the mean squared error between the predicted and the actual output.\n",
            "\n",
            "A fully connected network has an input layer, an output layer, and the weights for the connections between neurons in the network. The weights are the elements of the adjacency matrix of the network. The adjacency matrix is a symmetric matrix whose entries are 0 or 1.\n",
            "\n",
            "Deep belief networks are a sub-type of deep neural networks where\n",
            "\n",
            "the neurons in the network are connected to each other by weight connections,\n",
            "\n",
            "the connections between neurons are in the form of directed weighted graphs, and\n",
            "\n",
            "the network is trained to minimize the log likelihood of the network output given data.\n",
            "\n",
            "The structure of a deep belief network is a layered network. Each layer has an input, an output and a set of hidden neurons. The hidden neurons are connected to the input neurons. The connections between layers are in the form of directed weighted graphs and the connection weights are the adjacency matrices.\n",
            "\n",
            "Recurrent neural networks are a sub-type of deep neural networks where\n",
            "\n",
            "each layer has an input layer, an output layer, and a set of hidden neurons,\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QKg079qvt_W3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}